<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8">
    <title>AI/FaceVision.tsx - Documentation</title>


    <script src="scripts/prettify/prettify.js"></script>
    <script src="scripts/prettify/lang-css.js"></script>
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link href="styles/prettify.css" rel="stylesheet" type="text/css">
    <link href="styles/jsdoc.css" rel="stylesheet" type="text/css">
    <script defer src="scripts/nav.js"></script>

    <meta content="width=device-width, initial-scale=1.0" name="viewport">
</head>
<body>

<input class="nav-trigger" id="nav-trigger" type="checkbox"/>
<label class="navicon-button x" for="nav-trigger">
    <div class="navicon"></div>
</label>

<label class="overlay" for="nav-trigger"></label>

<nav>


    <h2><a href="index.html">Home</a></h2>
    <h3>Classes</h3>
    <ul>
        <li><a href="Action.html">Action</a></li>
        <li><a href="global.html#Gesture">Gesture</a></li>
        <li><a href="gestureRecognizer.html">gestureRecognizer</a></li>
        <li><a href="module-facialRecognition.faceLandmarker.html">faceLandmarker</a></li>
    </ul>
    <h3>Interfaces</h3>
    <ul>
        <li><a href="ActionAttributes.html">ActionAttributes</a></li>
        <li><a href="ActionCreationAttributes.html">ActionCreationAttributes</a></li>
        <li><a href="GestureAttributes.html">GestureAttributes</a></li>
        <li><a href="GestureCreationAttributes.html">GestureCreationAttributes</a></li>
        <li><a href="ToolbarItemType.html">ToolbarItemType</a></li>
    </ul>
    <h3>Global</h3>
    <ul>
        <li><a href="global.html#Home">Home</a></li>
        <li><a href="global.html#MinimalView">MinimalView</a></li>
        <li><a href="global.html#NavBar">NavBar</a></li>
        <li><a href="global.html#SourceItem">SourceItem</a></li>
        <li><a href="global.html#TopAppBar">TopAppBar</a></li>
        <li><a href="global.html#Webcam">Webcam</a></li>
        <li><a href="global.html#createAction">createAction</a></li>
        <li><a href="global.html#createGesture">createGesture</a></li>
        <li><a href="global.html#deleteAction">deleteAction</a></li>
        <li><a href="global.html#deleteGesture">deleteGesture</a></li>
        <li><a href="global.html#formatTime">formatTime</a></li>
        <li><a href="global.html#getAction">getAction</a></li>
        <li><a href="global.html#getAllActions">getAllActions</a></li>
        <li><a href="global.html#getAllGestures">getAllGestures</a></li>
        <li><a href="global.html#getGesture">getGesture</a></li>
        <li><a href="global.html#isBuild">isBuild</a></li>
        <li><a href="global.html#locDb">locDb</a></li>
        <li><a href="global.html#openWebpage">openWebpage</a></li>
        <li><a href="global.html#pressKey">pressKey</a></li>
        <li><a href="global.html#releaseKey">releaseKey</a></li>
        <li><a href="global.html#updateAction">updateAction</a></li>
        <li><a href="global.html#updateGesture">updateGesture</a></li>
    </ul>

</nav>

<div id="main">

    <h1 class="page-title">AI/FaceVision.tsx</h1>


    <section>
        <article>
            <pre class="prettyprint source linenums"><code>import {DrawingUtils, FaceLandmarker, FilesetResolver, NormalizedLandmark} from "@mediapipe/tasks-vision";
import {Dispatch, SetStateAction} from "react";

/**
 * Utility class for detecting and tracking facial landmarks in an image.
 *
 * @class
 * @memberof module:facialRecognition
 */
let faceLandmarker: FaceLandmarker;
/**
 * Creates a face detection and tracking system for webcam video.
 *
 * @param {HTMLVideoElement} video - The video element to capture webcam feed.
 * @param {HTMLCanvasElement} canvasElement - The HTML canvas element for rendering the video feed and overlays.
 * @param {Dispatch&lt;SetStateAction&lt;boolean>>} setGazeState - A state setter function for the gaze state.
 *
 * @returns {Object} - An object containing the following methods:
 *   - createFaceMeshRecognizer: A method to create a face mesh recognizer.
 *   - predictWebcam: A method to start predicting webcam feed and updating the gaze state.
 *   - setOverlay: A method to toggle the overlay option.
 *
 * @async
 * @returns {Promise&lt;void>}
 */
export default function FaceDetection(
    video: HTMLVideoElement,
    canvasElement: HTMLCanvasElement,
    setGazeState: Dispatch&lt;SetStateAction&lt;boolean>>
) {
    let overlay: boolean

    /**
     * Creates a face mesh recognizer.
     * @async
     * @returns {Promise&lt;void>}
     */
    const createFaceMeshRecognizer = async () => {
        const vision = await FilesetResolver.forVisionTasks(
            "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
        );
        faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
            baseOptions: {
                modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
                delegate: "GPU"
            },
            outputFaceBlendshapes: true,
            runningMode: "VIDEO",
            numFaces: 1
        });
        return;
    };

    const canvasCtx = canvasElement.getContext("2d");

    let lastVideoTime = -1;
    let results: any = undefined;
    const drawingUtils = new DrawingUtils(canvasCtx!);

    function isUserLookingAtCamera(tessellationLandmarks: NormalizedLandmark[]): boolean {
        // Calculate the centroid of the tessellation landmarks
        const centroidX = tessellationLandmarks.reduce((sum, point) => sum + point.x, 0) / tessellationLandmarks.length;
        const centroidY = tessellationLandmarks.reduce((sum, point) => sum + point.y, 0) / tessellationLandmarks.length;

        // You might need to adjust these thresholds based on your specific use case
        const horizontalThreshold = 0.05; // Adjust as needed
        const verticalThreshold = 0.05; // Adjust as needed

        // Check if the centroid is within a certain range to consider the user looking at the camera
        const isLookingAtCamera =
            centroidX > 0.5 - horizontalThreshold &amp;&amp;
            centroidX &lt; 0.5 + horizontalThreshold &amp;&amp;
            centroidY > 0.5 - verticalThreshold &amp;&amp;
            centroidY &lt; 0.5 + verticalThreshold;

        return isLookingAtCamera;
    }

    function setOverlay(value: boolean) {
        overlay = value
    }

    async function predictWebcam() {
        let startTimeMs = performance.now();
        if (lastVideoTime !== video.currentTime) {
            lastVideoTime = video.currentTime;
            results = faceLandmarker.detectForVideo(video, startTimeMs);
        }
        if (results.faceLandmarks) {
            for (const landmarks of results.faceLandmarks) {
                if (overlay) {
                    drawingUtils.drawConnectors(landmarks, FaceLandmarker.FACE_LANDMARKS_TESSELATION, {
                        color: "rgba(68,68,68,0.29)",
                        lineWidth: 1,
                    });
                } else if (canvasCtx != null) {
                    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
                }
                setGazeState(isUserLookingAtCamera(landmarks))
            }
        }

        window.requestAnimationFrame(predictWebcam);
    }

    return {
        createFaceMeshRecognizer,
        predictWebcam,
        setOverlay
    };
}</code></pre>
        </article>
    </section>


</div>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc3/jsdoc">JSDoc 4.0.2</a> on Sun Dec 10 2023 17:53:20
    GMT+0200 (It√§-Euroopan normaaliaika) using the <a href="https://github.com/clenemt/docdash">docdash</a> theme.
</footer>

<script>prettyPrint();</script>
<script src="scripts/polyfill.js"></script>
<script src="scripts/linenumber.js"></script>


</body>
</html>
